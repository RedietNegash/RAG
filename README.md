## **1. Document Query System**

## Overview
This project showcases an intelligent document query system using **LlamaIndex** that efficiently retrieves relevant sections from large text collections, such as books, and generates accurate responses with **Large Language Models (LLMs)**. The system incorporates **Sentence Retrieval**, which indexes documents at the sentence level to enhance precision in identifying the most relevant information, and **Auto-Merging**, which automatically combines contextually related sentences to produce cohesive answers. Additionally, a feedback mechanism is integrated to ensure continuous improvement in response quality, making the system more effective over time.

## Key Features
•	LlamaIndex Integration: Utilizes LlamaIndex for effective document indexing and retrieval, enabling fast and accurate access to relevant sections of large text collections.

•	Sentence Retrieval: Breaks down documents into individual sentences or smaller chunks, allowing for fine-grained querying and retrieval of specific information.

•	Auto-Merging: Automatically combines related sentences or text chunks into coherent, contextually unified responses, enhancing the clarity and completeness of the answers.

•	LLM-Powered Responses: Leverages Large Language Models (LLMs) to generate contextually accurate and insightful answers based on the retrieved information, providing users with high-quality responses.

•	Feedback Mechanism: Incorporates a feedback system to collect and analyze user input, enabling continuous improvement of response quality and system performance over time.


## Setup

### Install Dependencies

To set up the project, you'll need to install several Python packages. Use the following commands to ensure all necessary libraries are installed:

```bash
pip install trulens-eval llama-index llama-index-embeddings-huggingface torch sentence-transformers datasets generativeai trulens-providers-huggingface qdrant-client chromadb llama-index-vector-stores-chroma transformers
```

### Hugging Face Configuration

1. **Obtain Access Token:** Sign up or log in to your Hugging Face account and obtain an access token from the [Hugging Face Hub](https://huggingface.co/settings/tokens). This token will be used for authentication when accessing models and APIs.

2. **Initialize Hugging Face Model:** Configure the `HuggingFaceInferenceAPI` using your Hugging Face access token. This client will be used for generating embeddings and responses. Use the `HuggingFaceEmbedding` for generating embeddings and `HuggingFaceInferenceAPI` for generating responses from the model.

### LlamaIndex Setup

1. **Initialize LlamaIndex Components:** Set up LlamaIndex components, including document loaders, node parsers, and vector store indices. Configure the `Document`, `SimpleDirectoryReader`, `SentenceWindowNodeParser`, and `VectorStoreIndex` to handle document processing and retrieval.

2. **Node Parsing and Indexing:** Use `SentenceWindowNodeParser` or `HierarchicalNodeParser` to split documents into manageable chunks. Create and configure a `VectorStoreIndex` to store and retrieve these chunks efficiently.

### Vector Storage Setup

1. **Configure Vector Store:** Set up your vector storage using Chroma or any other supported vector store. Ensure it is properly initialized and connected to handle the storage and retrieval of embeddings.

### Document Processing

1. **Document Loading:** Use a document loader (e.g., `SimpleDirectoryReader`) to import documents.
2. **Chunking:** Apply a text chunking strategy (e.g., `SentenceWindowNodeParser` or `HierarchicalNodeParser`) to split documents into manageable chunks for embedding.

### Embedding and Storage

1. **Generate Embeddings:** Convert text chunks into embeddings using the `HuggingFaceEmbedding` model from LlamaIndex.
2. **Store Vectors:** Save these embeddings into the vector store. This includes creating and configuring the index for efficient retrieval.

### Query Handling

1. **Augment Queries:** Enhance user queries by integrating relevant context retrieved from the vector store. This step improves the quality of the responses.
2. **Generate Responses:** Use the Hugging Face model to generate responses based on the augmented query context. Configure the `HuggingFaceInferenceAPI` for this purpose.

### Auto-Merging Retrieval

1. **Setup Auto-Merging Retriever:** Configure the `AutoMergingRetriever` from LlamaIndex to handle merging of query results from various retrieval methods, enhancing the response quality with a combination of results.

### Evaluation

1. **Configure Evaluation:** Use `TruLens` to set up evaluation and feedback mechanisms. This includes configuring feedback mechanisms to assess the relevance and quality of responses generated by the system.






# 2. Retrieval-Augmented Generation (RAG) System for Research Papers

This project implements a **Retrieval-Augmented Generation (RAG)** system designed to process and generate insights from research papers. The system utilizes various libraries, including **datasets**, **langchain**, **sentence-transformers**, **qdrant-client**, and **google-generativeai**, to achieve efficient text retrieval and generation.

## Overview

The RAG system efficiently retrieves relevant sections from large text collections and generates accurate responses using state-of-the-art language models. Key functionalities include:

- **Text Processing**: Clean and normalize text using NLP techniques.
- **Embedding Creation**: Generate embeddings for text chunks using a **SentenceTransformer** model.
- **Vector Database**: Store and retrieve embeddings using **Qdrant**.
- **Query Handling**: Retrieve relevant text chunks based on user queries.
- **Response Generation**: Use **Google Generative AI** to formulate answers based on retrieved chunks.

## Key Features

- **Text Normalization**: Removes noise from abstracts and lemmatizes words.
- **Chunking**: Splits text into smaller, manageable pieces for better retrieval.
- **Embedding**: Converts text chunks into embeddings for efficient searching.
- **Retrieval**: Efficiently retrieves relevant information using cosine distance.
- **Generative Responses**: Generates contextually accurate answers to user queries.
- **Evaluation**: Measures the system's performance with standard metrics.

## Installation

To set up the project, install the required packages using pip:

```bash
pip install datasets
pip install langchain
pip install -U sentence-transformers
pip install qdrant-client
pip install google-generativeai
```

## Usage
## Document Processing

1. **Load Dataset**: Use the **datasets** library to load the research paper dataset.
2. **Process Text**: Clean and lemmatize the abstracts.
3. **Chunk Text**: Split the processed abstracts into smaller chunks.

## Embedding and Storage

1. **Generate Embeddings**: Convert text chunks into embeddings using a pre-trained model.
2. **Store in Qdrant**: Upload embeddings to **Qdrant** for efficient retrieval.

## Query Handling

1. **Retrieve Relevant Chunks**: Use vector similarity to find relevant text chunks based on the query.
2. **Generate Responses**: Use **Google Generative AI** to create answers based on the retrieved chunks.

## Evaluation

The system's performance can be evaluated using various metrics, including **accuracy**, **precision**, **recall**, and **F1-score**.


